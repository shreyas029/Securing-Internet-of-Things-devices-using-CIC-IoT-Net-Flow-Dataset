{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XAjAlA7V49b",
        "outputId": "b1591d3d-0c36-4cd1-9b2b-d0e446909553"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            " CICIoT2023  'README_csv - README.pdf'\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/MyDrive/CiC-DataSet/Complete_Dataset/csv\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification 34 class (33+1)"
      ],
      "metadata": {
        "id": "eTHnFx0sbeOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "\n",
        "DATASET_DIRECTORY = '/content/drive/MyDrive/CiC-DataSet/Complete_Dataset/csv/CICIoT2023'\n",
        "\n",
        "\n",
        "df_sets = [f for f in os.listdir(DATASET_DIRECTORY) if f.endswith('.csv')]\n",
        "df_sets.sort()\n",
        "\n",
        "\n",
        "training_sets = df_sets[:int(len(df_sets) * 0.8)]\n",
        "test_sets = df_sets[int(len(df_sets) * 0.8):]\n",
        "\n",
        "X_columns = [\n",
        "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration',\n",
        "    'Rate', 'Srate', 'Drate', 'fin_flag_number', 'syn_flag_number',\n",
        "    'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
        "    'ece_flag_number', 'cwr_flag_number', 'ack_count',\n",
        "    'syn_count', 'fin_count', 'urg_count', 'rst_count',\n",
        "    'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP',\n",
        "    'UDP', 'DHCP', 'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min',\n",
        "    'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Magnitue',\n",
        "    'Radius', 'Covariance', 'Variance', 'Weight',\n",
        "]\n",
        "y_column = 'label'\n",
        "\n",
        "scaler = StandardScaler()\n",
        "svm_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3)\n",
        "\n",
        "\n",
        "print(\"Fitting scaler on training data...\")\n",
        "all_labels = set()\n",
        "for train_set in tqdm(training_sets, desc=\"Scaling Training Sets\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, usecols=X_columns + [y_column], chunksize=5000):\n",
        "            scaler.partial_fit(chunk[X_columns])\n",
        "            all_labels.update(chunk[y_column].unique())\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {train_set}: {e}\")\n",
        "\n",
        "\n",
        "all_labels = sorted(all_labels)\n",
        "print(f\"Unique labels found: {all_labels}\")\n",
        "\n",
        "print(\"Scaler fitted successfully.\")\n",
        "\n",
        "print(\"Training SVM Model...\")\n",
        "for train_set in tqdm(training_sets, desc=\"Training SVM\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
        "    try:\n",
        "        data = pd.read_csv(file_path, usecols=X_columns + [y_column])\n",
        "        if data.empty:\n",
        "            print(f\"Skipping empty file: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        data[X_columns] = scaler.transform(data[X_columns])\n",
        "\n",
        "        svm_model.partial_fit(data[X_columns], data[y_column], classes=all_labels)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {train_set}: {e}\")\n",
        "\n",
        "print(\"SVM Model trained successfully.\")\n",
        "\n",
        "\n",
        "y_test, y_pred = [], []\n",
        "\n",
        "print(\"Evaluating SVM Model...\")\n",
        "for test_set in tqdm(test_sets, desc=\"Evaluating Test Sets\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, test_set)\n",
        "    try:\n",
        "        test_data = pd.read_csv(file_path, usecols=X_columns + [y_column])\n",
        "        if test_data.empty:\n",
        "            print(f\"Skipping empty file: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        test_data[X_columns] = scaler.transform(test_data[X_columns])\n",
        "\n",
        "        y_test.extend(test_data[y_column].values)\n",
        "        y_pred.extend(svm_model.predict(test_data[X_columns]))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {test_set}: {e}\")\n",
        "\n",
        "\n",
        "if len(y_test) == 0 or len(y_pred) == 0:\n",
        "    print(\"No predictions or labels collected. Check data integrity.\")\n",
        "else:\n",
        "    print(\"##### Evaluation Results #####\")\n",
        "    print('Accuracy Score = ', accuracy_score(y_test, y_pred))\n",
        "    print('Recall Score = ', recall_score(y_test, y_pred, average='macro'))\n",
        "    print('Precision Score = ', precision_score(y_test, y_pred, average='macro'))\n",
        "    print('F1 Score = ', f1_score(y_test, y_pred, average='macro'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70zhbT0PV-8R",
        "outputId": "3a699aee-979e-48e6-da29-392f8a342a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting scaler on training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Scaling Training Sets: 100%|██████████| 135/135 [07:30<00:00,  3.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique labels found: ['Backdoor_Malware', 'BenignTraffic', 'BrowserHijacking', 'CommandInjection', 'DDoS-ACK_Fragmentation', 'DDoS-HTTP_Flood', 'DDoS-ICMP_Flood', 'DDoS-ICMP_Fragmentation', 'DDoS-PSHACK_Flood', 'DDoS-RSTFINFlood', 'DDoS-SYN_Flood', 'DDoS-SlowLoris', 'DDoS-SynonymousIP_Flood', 'DDoS-TCP_Flood', 'DDoS-UDP_Flood', 'DDoS-UDP_Fragmentation', 'DNS_Spoofing', 'DictionaryBruteForce', 'DoS-HTTP_Flood', 'DoS-SYN_Flood', 'DoS-TCP_Flood', 'DoS-UDP_Flood', 'MITM-ArpSpoofing', 'Mirai-greeth_flood', 'Mirai-greip_flood', 'Mirai-udpplain', 'Recon-HostDiscovery', 'Recon-OSScan', 'Recon-PingSweep', 'Recon-PortScan', 'SqlInjection', 'Uploading_Attack', 'VulnerabilityScan', 'XSS']\n",
            "Scaler fitted successfully.\n",
            "Training SVM Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training SVM: 100%|██████████| 135/135 [12:54<00:00,  5.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model trained successfully.\n",
            "Evaluating SVM Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Sets: 100%|██████████| 34/34 [02:24<00:00,  4.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Evaluation Results #####\n",
            "Accuracy Score =  0.7871270089508278\n",
            "Recall Score =  0.42767336250608096\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision Score =  0.5286743006990764\n",
            "F1 Score =  0.43372292838904086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification 8 (7+1)"
      ],
      "metadata": {
        "id": "eCTtv1zyJ7v6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "DATASET_DIRECTORY = '/content/drive/MyDrive/CiC-DataSet/Complete_Dataset/csv/CICIoT2023'\n",
        "X_columns = [\n",
        "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate',\n",
        "    'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
        "    'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count',\n",
        "    'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP',\n",
        "    'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT',\n",
        "    'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight',\n",
        "]\n",
        "y_column = 'label'\n",
        "\n",
        "dict_7classes = {\n",
        "    'DDoS-RSTFINFlood': 'DDoS', 'DDoS-PSHACK_Flood': 'DDoS', 'DDoS-SYN_Flood': 'DDoS',\n",
        "    'DDoS-UDP_Flood': 'DDoS', 'DDoS-TCP_Flood': 'DDoS', 'DDoS-ICMP_Flood': 'DDoS',\n",
        "    'DDoS-SynonymousIP_Flood': 'DDoS', 'DDoS-ACK_Fragmentation': 'DDoS',\n",
        "    'DDoS-UDP_Fragmentation': 'DDoS', 'DDoS-ICMP_Fragmentation': 'DDoS',\n",
        "    'DDoS-SlowLoris': 'DDoS', 'DDoS-HTTP_Flood': 'DDoS', 'DoS-UDP_Flood': 'DoS',\n",
        "    'DoS-SYN_Flood': 'DoS', 'DoS-TCP_Flood': 'DoS', 'DoS-HTTP_Flood': 'DoS',\n",
        "    'Mirai-greeth_flood': 'Mirai', 'Mirai-greip_flood': 'Mirai', 'Mirai-udpplain': 'Mirai',\n",
        "    'Recon-PingSweep': 'Recon', 'Recon-OSScan': 'Recon', 'Recon-PortScan': 'Recon',\n",
        "    'VulnerabilityScan': 'Recon', 'Recon-HostDiscovery': 'Recon', 'DNS_Spoofing': 'Spoofing',\n",
        "    'MITM-ArpSpoofing': 'Spoofing', 'BenignTraffic': 'Benign', 'BrowserHijacking': 'Web',\n",
        "    'Backdoor_Malware': 'Web', 'XSS': 'Web', 'Uploading_Attack': 'Web', 'SqlInjection': 'Web',\n",
        "    'CommandInjection': 'Web', 'DictionaryBruteForce': 'BruteForce'\n",
        "}\n",
        "\n",
        "df_sets = [f for f in os.listdir(DATASET_DIRECTORY) if f.endswith('.csv')]\n",
        "\n",
        "training_sets = df_sets[:int(len(df_sets) * 0.8)]\n",
        "test_sets = df_sets[int(len(df_sets) * 0.8):]\n",
        "\n",
        "print(\"Fitting scaler on training data...\")\n",
        "scaler = StandardScaler()\n",
        "for train_set in tqdm(training_sets, desc=\"Fitting Scaler\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, chunksize=5000, usecols=X_columns + [y_column]):\n",
        "            scaler.partial_fit(chunk[X_columns])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "print(\"Scaler fitted successfully.\")\n",
        "\n",
        "print(\"Training the SVM (using SGDClassifier)...\")\n",
        "svm_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3)\n",
        "all_labels = sorted(set(dict_7classes.values()))\n",
        "\n",
        "for train_set in tqdm(training_sets, desc=\"Training SVM\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, chunksize=5000, usecols=X_columns + [y_column]):\n",
        "            chunk[X_columns] = scaler.transform(chunk[X_columns])\n",
        "            chunk[y_column] = chunk[y_column].map(dict_7classes)\n",
        "            svm_model.partial_fit(chunk[X_columns], chunk[y_column], classes=all_labels)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "print(\"SVM Model trained successfully.\")\n",
        "\n",
        "print(\"Evaluating SVM Model...\")\n",
        "y_test, y_pred = [], []\n",
        "\n",
        "for test_set in tqdm(test_sets, desc=\"Evaluating Test Sets\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, test_set)\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, chunksize=5000, usecols=X_columns + [y_column]):\n",
        "            chunk[X_columns] = scaler.transform(chunk[X_columns])\n",
        "            chunk[y_column] = chunk[y_column].map(dict_7classes)\n",
        "            y_test.extend(chunk[y_column].values)\n",
        "            y_pred.extend(svm_model.predict(chunk[X_columns]))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "\n",
        "if len(y_test) > 0 and len(y_pred) > 0:\n",
        "    print(\"##### Evaluation Results #####\")\n",
        "    print(\"Accuracy Score = \", accuracy_score(y_test, y_pred))\n",
        "    print(\"Recall Score = \", recall_score(y_test, y_pred, average='macro'))\n",
        "    print(\"Precision Score = \", precision_score(y_test, y_pred, average='macro'))\n",
        "    print(\"F1 Score = \", f1_score(y_test, y_pred, average='macro'))\n",
        "else:\n",
        "    print(\"No predictions or labels collected. Check data integrity.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqRFwAG8KCMm",
        "outputId": "f592f5e2-4b08-4ae9-8341-5771ff7f2205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting scaler on training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitting Scaler: 100%|██████████| 135/135 [05:30<00:00,  2.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler fitted successfully.\n",
            "Training the SVM (using SGDClassifier)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training SVM: 100%|██████████| 135/135 [08:25<00:00,  3.74s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model trained successfully.\n",
            "Evaluating SVM Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Sets: 100%|██████████| 34/34 [03:07<00:00,  5.51s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Evaluation Results #####\n",
            "Accuracy Score =  0.8230735478876973\n",
            "Recall Score =  0.46898625413482875\n",
            "Precision Score =  0.6774741457451166\n",
            "F1 Score =  0.5017935247751959\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification 2 class (1+1)"
      ],
      "metadata": {
        "id": "gm3iKmEC8AUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from tqdm import tqdm\n",
        "\n",
        "DATASET_DIRECTORY = '/content/drive/MyDrive/CiC-DataSet/Complete_Dataset/csv/CICIoT2023'\n",
        "X_columns = [\n",
        "    'flow_duration', 'Header_Length', 'Protocol Type', 'Duration', 'Rate', 'Srate', 'Drate',\n",
        "    'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number',\n",
        "    'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'urg_count',\n",
        "    'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP',\n",
        "    'ARP', 'ICMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT',\n",
        "    'Number', 'Magnitue', 'Radius', 'Covariance', 'Variance', 'Weight',\n",
        "]\n",
        "y_column = 'label'\n",
        "\n",
        "dict_2classes = {\n",
        "    'DDoS-RSTFINFlood': 'Attack', 'DDoS-PSHACK_Flood': 'Attack', 'DDoS-SYN_Flood': 'Attack',\n",
        "    'DDoS-UDP_Flood': 'Attack', 'DDoS-TCP_Flood': 'Attack', 'DDoS-ICMP_Flood': 'Attack',\n",
        "    'DDoS-SynonymousIP_Flood': 'Attack', 'DDoS-ACK_Fragmentation': 'Attack',\n",
        "    'DDoS-UDP_Fragmentation': 'Attack', 'DDoS-ICMP_Fragmentation': 'Attack',\n",
        "    'DDoS-SlowLoris': 'Attack', 'DDoS-HTTP_Flood': 'Attack', 'DoS-UDP_Flood': 'Attack',\n",
        "    'DoS-SYN_Flood': 'Attack', 'DoS-TCP_Flood': 'Attack', 'DoS-HTTP_Flood': 'Attack',\n",
        "    'Mirai-greeth_flood': 'Attack', 'Mirai-greip_flood': 'Attack', 'Mirai-udpplain': 'Attack',\n",
        "    'Recon-PingSweep': 'Attack', 'Recon-OSScan': 'Attack', 'Recon-PortScan': 'Attack',\n",
        "    'VulnerabilityScan': 'Attack', 'Recon-HostDiscovery': 'Attack', 'DNS_Spoofing': 'Attack',\n",
        "    'MITM-ArpSpoofing': 'Attack', 'BenignTraffic': 'Benign', 'BrowserHijacking': 'Attack',\n",
        "    'Backdoor_Malware': 'Attack', 'XSS': 'Attack', 'Uploading_Attack': 'Attack',\n",
        "    'SqlInjection': 'Attack', 'CommandInjection': 'Attack', 'DictionaryBruteForce': 'Attack'\n",
        "}\n",
        "\n",
        "df_sets = [f for f in os.listdir(DATASET_DIRECTORY) if f.endswith('.csv')]\n",
        "\n",
        "training_sets = df_sets[:int(len(df_sets) * 0.8)]\n",
        "test_sets = df_sets[int(len(df_sets) * 0.8):]\n",
        "\n",
        "print(\"Fitting scaler on training data...\")\n",
        "scaler = StandardScaler()\n",
        "for train_set in tqdm(training_sets, desc=\"Fitting Scaler\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, chunksize=5000, usecols=X_columns + [y_column]):\n",
        "            scaler.partial_fit(chunk[X_columns])\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "print(\"Scaler fitted successfully.\")\n",
        "\n",
        "print(\"Training the SVM (using SGDClassifier)...\")\n",
        "svm_model = SGDClassifier(loss='hinge', max_iter=1000, tol=1e-3)\n",
        "classes = ['Attack', 'Benign']\n",
        "\n",
        "for train_set in tqdm(training_sets, desc=\"Training SVM\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, train_set)\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, chunksize=5000, usecols=X_columns + [y_column]):\n",
        "            chunk[X_columns] = scaler.transform(chunk[X_columns])\n",
        "            chunk[y_column] = chunk[y_column].map(dict_2classes)\n",
        "            svm_model.partial_fit(chunk[X_columns], chunk[y_column], classes=classes)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "print(\"SVM Model trained successfully.\")\n",
        "\n",
        "print(\"Evaluating SVM Model...\")\n",
        "y_test, y_pred = [], []\n",
        "\n",
        "for test_set in tqdm(test_sets, desc=\"Evaluating Test Sets\"):\n",
        "    file_path = os.path.join(DATASET_DIRECTORY, test_set)\n",
        "    try:\n",
        "        for chunk in pd.read_csv(file_path, chunksize=5000, usecols=X_columns + [y_column]):\n",
        "            chunk[X_columns] = scaler.transform(chunk[X_columns])\n",
        "            chunk[y_column] = chunk[y_column].map(dict_2classes)\n",
        "            y_test.extend(chunk[y_column].values)\n",
        "            y_pred.extend(svm_model.predict(chunk[X_columns]))\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {file_path}: {e}\")\n",
        "\n",
        "if len(y_test) > 0 and len(y_pred) > 0:\n",
        "    print(\"##### Evaluation Results #####\")\n",
        "    print(\"Accuracy Score = \", accuracy_score(y_test, y_pred))\n",
        "    print(\"Recall Score = \", recall_score(y_test, y_pred, average='macro'))\n",
        "    print(\"Precision Score = \", precision_score(y_test, y_pred, average='macro'))\n",
        "    print(\"F1 Score = \", f1_score(y_test, y_pred, average='macro'))\n",
        "else:\n",
        "    print(\"No predictions or labels collected. Check data integrity.\")\n"
      ],
      "metadata": {
        "id": "zLHdJrYp7_69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97930175-3997-4439-ba7e-cd8ccc90ad6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting scaler on training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fitting Scaler: 100%|██████████| 135/135 [04:58<00:00,  2.21s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler fitted successfully.\n",
            "Training the SVM (using SGDClassifier)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training SVM: 100%|██████████| 135/135 [06:29<00:00,  2.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Model trained successfully.\n",
            "Evaluating SVM Model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Test Sets: 100%|██████████| 34/34 [03:01<00:00,  5.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "##### Evaluation Results #####\n",
            "Accuracy Score =  0.9871150942427299\n",
            "Recall Score =  0.8353270133620774\n",
            "Precision Score =  0.8718356978563582\n",
            "F1 Score =  0.8525845306892493\n"
          ]
        }
      ]
    }
  ]
}